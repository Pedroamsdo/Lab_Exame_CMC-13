{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Laboratório Exame\n",
    "## Equipe:\n",
    "## Pedro Anacleto Martins Senna De Oliveira\n",
    "## André Luiz de Melo Thissen\n",
    "## Nikollas da Silva Antes\n",
    "## Pedro Luchiari de Carvalho"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importação de bibliotecas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importando DataSets\n",
    "- Datasets foram unidos para que o Label Encoding fosse o mesmo para categorias iguais, uma vez que o dataset de treinamento tinha mais categoria."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "outputs": [],
   "source": [
    "teste = pd.read_csv('exame_cmc13_dados_teste.csv',sep=';',on_bad_lines='skip')\n",
    "treinamento = pd.read_csv('exame_cmc13_dados_treinamento.csv',sep=';',on_bad_lines='skip')\n",
    "total = pd.concat([treinamento,teste])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Verificação de que há elementos nulos com posterior remoção desses elementos."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(total.isnull().values.any())\n",
    "total = total.dropna(how='any')\n",
    "print(total.isnull().values.any())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Verificação da porcentagem de dados de teste e treinamento a fim da separação do dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=131179, step=1)\n",
      "RangeIndex(start=0, stop=32795, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(treinamento.index)\n",
    "print(teste.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Remoção de colunas que foram tidas como irrelevantes para a classificação dos filmes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "outputs": [],
   "source": [
    "total = total.drop(columns='Unnamed: 0.1')\n",
    "total = total.drop(columns='Unnamed: 0')\n",
    "total = total.drop(columns='user_id')\n",
    "total = total.drop(columns='isbn')\n",
    "total = total.drop(columns='year_of_publication')\n",
    "total= total.drop(columns='img_l')\n",
    "total= total.drop(columns='Language')\n",
    "total = total.drop(columns='Category')\n",
    "total= total.drop(columns='city')\n",
    "total = total.drop(columns='state')\n",
    "total.loc[:,'age'] =total.loc[:,'age'].astype(int)\n",
    "total = total [['country','publisher','age','book_title','book_author','rating']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Separação dos dados em atributos independentes e dependente."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "outputs": [],
   "source": [
    "variables =pd.DataFrame(total.iloc[:,0:5].values)\n",
    "results = total.iloc[:,5].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Label Encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "outputs": [
    {
     "data": {
      "text/plain": "          0    1   2    3    4\n0       172   31  34  839  205\n1       172   31  34  839  205\n2       172   31  35  839  205\n3       172   31  29  839  205\n4       172   31  31  839  205\n...     ...  ...  ..  ...  ...\n156456  172   74  59  257  181\n156457  172   96  34  798   22\n156458  172   31  25  795  167\n156459  172    5  55  418   13\n156460  172  118  34   43  131\n\n[156461 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>172</td>\n      <td>31</td>\n      <td>34</td>\n      <td>839</td>\n      <td>205</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>172</td>\n      <td>31</td>\n      <td>34</td>\n      <td>839</td>\n      <td>205</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>172</td>\n      <td>31</td>\n      <td>35</td>\n      <td>839</td>\n      <td>205</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>172</td>\n      <td>31</td>\n      <td>29</td>\n      <td>839</td>\n      <td>205</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>172</td>\n      <td>31</td>\n      <td>31</td>\n      <td>839</td>\n      <td>205</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>156456</th>\n      <td>172</td>\n      <td>74</td>\n      <td>59</td>\n      <td>257</td>\n      <td>181</td>\n    </tr>\n    <tr>\n      <th>156457</th>\n      <td>172</td>\n      <td>96</td>\n      <td>34</td>\n      <td>798</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>156458</th>\n      <td>172</td>\n      <td>31</td>\n      <td>25</td>\n      <td>795</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>156459</th>\n      <td>172</td>\n      <td>5</td>\n      <td>55</td>\n      <td>418</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>156460</th>\n      <td>172</td>\n      <td>118</td>\n      <td>34</td>\n      <td>43</td>\n      <td>131</td>\n    </tr>\n  </tbody>\n</table>\n<p>156461 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables.loc[:, 0] =  LabelEncoder().fit_transform(variables.iloc[:, 0])\n",
    "variables.loc[:,1] = LabelEncoder().fit_transform((variables.iloc[:,1]))\n",
    "variables.loc[:,3] = LabelEncoder().fit_transform((variables.iloc[:,3]))\n",
    "variables.loc[:,4] = LabelEncoder().fit_transform((variables.iloc[:,4]))\n",
    "variables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Split em dados de treinamento e teste"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato dos dados de treinamento:  (117345, 5)\n",
      "Formato dos dados de teste:  (39116, 5)\n",
      "Formato dos dados de treinamento:  (117345,)\n",
      "Formato dos dados de teste:  (39116,)\n"
     ]
    }
   ],
   "source": [
    "variables_treinamento, variable_test, results_treinamento, results_teste = train_test_split(variables, results, test_size = 0.25, random_state = 0)\n",
    "print('Formato dos dados de treinamento: ',variables_treinamento.shape)\n",
    "print('Formato dos dados de teste: ',variable_test.shape)\n",
    "print('Formato dos dados de treinamento: ',results_treinamento.shape)\n",
    "print('Formato dos dados de teste: ',results_teste.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Normalização dos dados"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "variables_treinamento = sc.fit_transform(variables_treinamento)\n",
    "variable_test = sc.transform(variable_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation - Redes Neurais do Tipo MLP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1174/1174 [==============================] - 2s 1ms/step - loss: 1.6500 - accuracy: 0.5935\n",
      "Epoch 2/20\n",
      "1174/1174 [==============================] - 1s 975us/step - loss: 1.4342 - accuracy: 0.5938\n",
      "Epoch 3/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4329 - accuracy: 0.5938\n",
      "Epoch 4/20\n",
      "1174/1174 [==============================] - 1s 991us/step - loss: 1.4328 - accuracy: 0.5938\n",
      "Epoch 5/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4328 - accuracy: 0.5938\n",
      "Epoch 6/20\n",
      "1174/1174 [==============================] - 1s 974us/step - loss: 1.4327 - accuracy: 0.5938\n",
      "Epoch 7/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4326 - accuracy: 0.5938\n",
      "Epoch 8/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4328 - accuracy: 0.5938\n",
      "Epoch 9/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4327 - accuracy: 0.5938\n",
      "Epoch 10/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4327 - accuracy: 0.5938\n",
      "Epoch 11/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4328 - accuracy: 0.5938\n",
      "Epoch 12/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4328 - accuracy: 0.5938\n",
      "Epoch 13/20\n",
      "1174/1174 [==============================] - 2s 2ms/step - loss: 1.4327 - accuracy: 0.5938\n",
      "Epoch 14/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4326 - accuracy: 0.5938\n",
      "Epoch 15/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4327 - accuracy: 0.5938\n",
      "Epoch 16/20\n",
      "1174/1174 [==============================] - 2s 1ms/step - loss: 1.4327 - accuracy: 0.5938\n",
      "Epoch 17/20\n",
      "1174/1174 [==============================] - 2s 1ms/step - loss: 1.4327 - accuracy: 0.5938\n",
      "Epoch 18/20\n",
      "1174/1174 [==============================] - 2s 2ms/step - loss: 1.4327 - accuracy: 0.5938\n",
      "Epoch 19/20\n",
      "1174/1174 [==============================] - 2s 1ms/step - loss: 1.4326 - accuracy: 0.5938\n",
      "Epoch 20/20\n",
      "1174/1174 [==============================] - 1s 1ms/step - loss: 1.4327 - accuracy: 0.5938\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2c9aa2d13a0>"
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador = Sequential()\n",
    "classificador.add(Dense(2, kernel_initializer = 'uniform', activation = 'relu', input_dim = 5))\n",
    "classificador.add(Dense( 3, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classificador.add(Dense( 11, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "classificador.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "classificador.fit(variables_treinamento, results_treinamento, batch_size = 100, epochs= 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 1s 714us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.5873554 , 0.00157073, 0.00274408, ..., 0.10587692, 0.07205503,\n        0.0797283 ],\n       [0.6004832 , 0.00137717, 0.00244187, ..., 0.10381611, 0.06995583,\n        0.07734184],\n       [0.5763359 , 0.0017504 , 0.00302064, ..., 0.10754726, 0.07379507,\n        0.08170936],\n       ...,\n       [0.61464226, 0.00119116, 0.00214665, ..., 0.10150845, 0.06766169,\n        0.07473799],\n       [0.61155564, 0.00122981, 0.00220843, ..., 0.10201897, 0.06816437,\n        0.07530817],\n       [0.5917018 , 0.00150427, 0.00264084, ..., 0.10520307, 0.07136309,\n        0.07894125]], dtype=float32)"
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsPred = classificador.predict(variable_test)\n",
    "resultsPred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Análise dos Resultados"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23189     0     0     0     0     0     0     0     0     0     0]\n",
      " [   60     0     0     0     0     0     0     0     0     0     0]\n",
      " [  102     0     0     0     0     0     0     0     0     0     0]\n",
      " [  203     0     0     0     0     0     0     0     0     0     0]\n",
      " [  291     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 1420     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 1196     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 2706     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 4095     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 2872     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 2982     0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.5928264648737089"
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetorPred = np.zeros((39116,1)).astype(int)\n",
    "cm = confusion_matrix(results_teste, vetorPred)\n",
    "print(cm)\n",
    "accuracy_score(results_teste,vetorPred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation - Random Forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "outputs": [
    {
     "data": {
      "text/plain": "array([4, 2, 3, ..., 3, 5, 1])"
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 20)\n",
    "rf.fit(variables_treinamento, results_treinamento)\n",
    "rfPred = rf.predict(variable_test)\n",
    "rfPred = rfPred.astype(int)\n",
    "rfPred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Análise dos Resultados"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4120 3256 4287 4711 3083 1709 1094  557  283   81    8]\n",
      " [  19   11    7    5   10    3    2    3    0    0    0]\n",
      " [  24   18   18   18   14    3    2    1    4    0    0]\n",
      " [  26   25   36   53   35   12    8    4    4    0    0]\n",
      " [  49   36   59   74   31   24    4   10    4    0    0]\n",
      " [ 197  181  271  347  189  116   65   34   18    2    0]\n",
      " [ 168  160  240  249  177   96   50   29   23    4    0]\n",
      " [ 340  354  504  627  387  240  136   70   43    5    0]\n",
      " [ 513  469  767  930  627  346  236  131   58   18    0]\n",
      " [ 385  326  456  581  501  285  165  104   46   22    1]\n",
      " [ 352  296  451  607  481  332  210  138   71   40    4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.11639738214541365"
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(results_teste, rfPred)\n",
    "print(cm)\n",
    "accuracy_score(results_teste,rfPred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusão\n",
    " - Percebe-se que as redes neurais tiveram um desempenho melhor uma vez que tiveram a taxa de acerto maior. Outro fator importante a ser mencionado, trata-se que a rede neurai, nesse exemplo, foi enviezada de tal forma que todas as predições dela tendiam a zero, o que é fator devido aos dados fornecidos.A rede neurai possui uma diferença no fato de que dão probalidade para cada tipo de saída com base nos inputs, trazendo uma precisão maior e uma taxa de aprendizagem melhor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}